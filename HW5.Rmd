---
title: "HW5"
author: "Lan Shui"
date: "6/5/2022"
output: pdf_document
---

```{r setup, include=FALSE}
### Setting up the packages
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(simulator)
library(glmnet)
library(nycflights13)
```

# Responses 
## Problem 1
## -1.1
```{r, include=T}
output<-flights %>% 
  filter(origin == "JFK", dest == "SEA") %>%
  group_by(carrier) %>%
  summarize(mean.delay = mean(arr_delay, na.rm = TRUE),
            sd.delay = sd(arr_delay, na.rm = TRUE)) %>%
  arrange(mean.delay)
output
```
## -1.2
```{r, include=T}
output %>% left_join(airlines)
```
## -1.3
```{r, include=T}
# we first filter x from flights data frame with tailnum and carrier variables
# with criteria that the flights is from New York (JFK) to Seattle (SEA) in 2013
x<-flights %>% 
  filter(origin == "JFK", dest == "SEA",year == 2013) %>%
  select(tailnum,carrier)
# we then filter y from planes data frame with tailnum and year variables
y<-planes %>%
  select(tailnum,year)
# we join x and y by tailnum
joinxy<-x %>%
  left_join(y)
joinxy %>% 
  group_by(carrier) %>%
  summarize(mean.age = mean(year, na.rm = TRUE))
# We noticed that there is a lack of information for the year for carrier AA
```

## Problem 2
The simulation code is in appendix
```{r, include=F}
make_mv_linear_model <- function(n, beta, sigma_sq) 
{
  new_model(          # Model constructor
    name = "mv_lm", 
    label = sprintf("n = %s, beta = %s, sigma_sq = %s", n, paste(beta, collapse = " "), sigma_sq),
    params = list(beta = beta, sigma_sq = sigma_sq, n = n),
    simulate = function(n, beta, sigma_sq, nsim)
    {
      sim_list =  map(1:nsim, function(ii){
        p = length(beta)
        x <- matrix(rnorm(n*p, mean = 0, sd = 1), nrow = n)
        y <- x %*% beta + rnorm(n, 0, sqrt(sigma_sq))
        list("x" = x, "y" = y)})
      
      return(sim_list) 
    }
  )
}

lse <- new_method("lse", "LSE  w/o intrcpt",
                  method = function(model, draw) {
                    yy <- draw$y
                    xx <- draw$x
                    fit <- lm(yy ~ xx - 1)
                    list(betahat = fit$coef)
                  })

lasso <- new_method("lasso", "LASSO  w/o intrcpt",
                  method = function(model, draw) {
                    yy <- draw$y
                    xx <- draw$x
                    
                    cv.out <- cv.glmnet(xx,yy,alpha=1,nfolds=5)   # Cross validated choice of the penalty
                    optimal_lambda = cv.out$lambda[which.min(cv.out$cvm)]
                    beta_est = as.numeric(glmnet(xx,yy,alpha=1, lambda = optimal_lambda)$beta)  # Run model 
                    
                    list(betahat = beta_est)
                  })

sq_err <- new_metric("l2", 
                     "Squared error",
                     metric = function(model, out) {
                       sum((out$betahat - model$beta)^2)   # beta is a scalar
                     })

sup_rec <- new_metric("sup_rec",
                      "support recovery",
                      metric = function(model, out){
                        sum=0
                        for (i in seq_along(out$betahat)){
                          if (out$betahat[i]==0 & model$beta[i]==0) {
                            sum=sum+1
                          }
                            else if (out$betahat[i]>0 & model$beta[i]>0) {
                              sum=sum+1
                            }
                              else {
                                sum=sum
                              }
                        }
                        return(sum/length(out$betahat))
                      })

sim <- new_simulation("ls_vs_lasso", "Least quares vs LASSO") %>%
  generate_model(make_mv_linear_model,                               # Specify data generation function
                 n = 40,                                     # Data generation parameter 1
                 sigma_sq = as.list(c(1,4,7,10)),
                 beta = c(0,1,2,0,0),
                 vary_along = c("sigma_sq")) %>%
  simulate_from_model(nsim = 15)  %>%
  run_method(list(lse,lasso)) %>%
  evaluate(list(sq_err,sup_rec))
```

```{r, include=T}
#Plot the result
sim %>% plot_eval_by(metric_name = "l2", varying = "sigma_sq")
```
The mean squared errors of LASSO linear model and simple linear model are approximately the same when the standard deviation of error term is less than 7.5. When the standard deviation of error term is larger than 7.5, LASSO has smaller mean squared errors compared to simple linear model.

```{r, include=T}
sim %>% plot_eval_by(metric_name = "sup_rec", varying = "sigma_sq")
```
For simple linear model, its support recovery is 0.4 along different standard deviation values, which means the number of betas it can correctly estimate to be equal to or different from zero is 2. For LASSO, the support recovery is around 0.7 when standard deviation gets larger, which means the number of betas it can correctly estimate to be equal to or different from zero is 3.5.

\pagebreak

## Code Appendix

```{r, ref.label=knitr::all_labels()[5],echo=TRUE,eval=FALSE}
```
 